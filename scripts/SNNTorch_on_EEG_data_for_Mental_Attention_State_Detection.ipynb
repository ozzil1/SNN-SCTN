{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0vN2UZv1H33A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from collections import OrderedDict\n",
    "import snntorch as snn\n",
    "\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def gpu_mem_state():\n",
    "  # Print out the GPU memory usage\n",
    "  print(\"Memory allocated:\", torch.cuda.memory_allocated() / 1024**3, \"GB\")\n",
    "  print(\"Max memory allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\")\n",
    "\n",
    "gpu_mem_state()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkYUK1DAYYGC",
    "outputId": "e96fe856-2791-4bae-eb82-060281891e88"
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.0 GB\n",
      "Max memory allocated: 0.0 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class ResonatorSpikes:\n",
    "\n",
    "    def __init__(self, clk_freq, resonator_freq, spikes_path):\n",
    "        self.clk_freq = clk_freq\n",
    "        self.resonator_freq = resonator_freq\n",
    "        self.events = None\n",
    "        self._load_spikes(spikes_path)\n",
    "\n",
    "    def _load_spikes(self, spikes_path):\n",
    "        spikes_array = np.load(spikes_path)['spikes']\n",
    "        # if the file is already events based spikes\n",
    "        if np.max(spikes_array) > 1:\n",
    "            self.events = spikes_array\n",
    "        else:\n",
    "            self.events = np.where(spikes_array == 1)[0]\n",
    "\n",
    "    def spectrogram(self, window_ms):\n",
    "        window = int(self.clk_freq/1000 * window_ms)\n",
    "        N = self.events[-1] // window + 1\n",
    "        bins = np.zeros(N, dtype=int)\n",
    "        unique_indices, counts = np.unique(np.array(self.events) // window, return_counts=True)\n",
    "        bins[unique_indices] = counts\n",
    "        return bins\n",
    "\n",
    "\n",
    "class ChannelSpikes:\n",
    "\n",
    "    def __init__(self, base_folder, channel_name):\n",
    "        self.channel_name = channel_name\n",
    "        self.resonators_output = OrderedDict({})\n",
    "        self._load_resonators_output(base_folder)\n",
    "\n",
    "    def _load_resonators_output(self, base_folder):\n",
    "        channel_folder = base_folder / self.channel_name\n",
    "        for clk_freq in os.listdir(channel_folder):\n",
    "            clk_folder = channel_folder / clk_freq\n",
    "            for spikes in os.listdir(clk_folder):\n",
    "                resonator_freq = spikes[:-4]\n",
    "                self.resonators_output[resonator_freq] = ResonatorSpikes(int(clk_freq), float(resonator_freq), f'{clk_folder}/{spikes}')\n",
    "\n",
    "class SignalSpikes:\n",
    "\n",
    "    def __init__(self, signal_folder, label):\n",
    "        self.label = label\n",
    "        self.channels = OrderedDict({\n",
    "            channel: ChannelSpikes(signal_folder, channel)\n",
    "            for channel in os.listdir(signal_folder)\n",
    "        })\n",
    "\n",
    "\n",
    "class Trial:\n",
    "\n",
    "    def __init__(self, base_folder, trial):\n",
    "        self.trial = trial\n",
    "        self.base_folder = Path(f'{base_folder}/{trial}')\n",
    "\n",
    "    def load(self, minute):\n",
    "        # make sure it's in string format.\n",
    "        minute = str(minute)\n",
    "        for label in os.listdir(self.base_folder):\n",
    "            for m in os.listdir(self.base_folder / label):\n",
    "                if m == minute:\n",
    "                    return SignalSpikes(self.base_folder / label / m, label=label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "@nb.njit\n",
    "def fast_filter_spikes(ts_spikes, start_idx, end_idx):\n",
    "    filtered_spikes = np.empty_like(ts_spikes)\n",
    "    count = 0\n",
    "    for spike in ts_spikes:\n",
    "        if start_idx <= spike < end_idx:\n",
    "            filtered_spikes[count] = spike\n",
    "            count += 1\n",
    "    return filtered_spikes[:count]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EEGMentalSpikesDataset(Dataset):\n",
    "    def __init__(self, trials: List[Trial], minutes: List[int], time_sample_s: float, labels_mapper: Dict[str, int]):\n",
    "        self.time_sample_s = time_sample_s\n",
    "        self.labels_mapper = labels_mapper\n",
    "\n",
    "        self.samples_per_minute = int(60 / time_sample_s)\n",
    "        self.samples_per_trial = self.samples_per_minute * len(minutes)\n",
    "        self.length = len(trials) * self.samples_per_trial\n",
    "\n",
    "        self.loaded_spikes = {\n",
    "            f'{i}-{j}': trial.load(minute)\n",
    "            for i, trial in enumerate(trials)\n",
    "            for j, minute in enumerate(minutes)\n",
    "        }\n",
    "        # get resonators clk frequencies.\n",
    "        signal4example = next(iter(self.loaded_spikes.values()))\n",
    "        resonators = next(iter(signal4example.channels.values())).resonators_output\n",
    "\n",
    "        self.map_channel_to_id = {ch: i for i, ch in enumerate(signal4example.channels.keys())}\n",
    "        self.map_resonator_to_id = {f: i for i, f in enumerate(resonators.keys())}\n",
    "\n",
    "        clk_freq = list(set(map(lambda x: x.clk_freq, resonators.values())))\n",
    "        # least common multiplier of the resonators is network clk frequency\n",
    "        self.network_clk = np.lcm.reduce(clk_freq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        num_rows = 50_000\n",
    "\n",
    "        trial_id = id // self.samples_per_trial\n",
    "        minute_id = (id % self.samples_per_trial) // self.samples_per_minute\n",
    "        sample = ((id % self.samples_per_trial) % self.samples_per_minute)\n",
    "\n",
    "        spike_signal = self.loaded_spikes[f'{trial_id}-{minute_id}']\n",
    "\n",
    "        label = self.labels_mapper[spike_signal.label]\n",
    "\n",
    "        # ordered dict to numpy array\n",
    "        # Determine the size of the resulting array\n",
    "        result = -np.ones((num_rows, 2), dtype=np.int64)\n",
    "        result_index = 0\n",
    "\n",
    "        for ch, channel_spikes in spike_signal.channels.items():\n",
    "            ch_id = self.map_channel_to_id[ch]\n",
    "            for f, resonator in channel_spikes.resonators_output.items():\n",
    "                resonator_id = self.map_resonator_to_id[f]\n",
    "                ticks_in_sample = int(self.time_sample_s * resonator.clk_freq)\n",
    "                ts_spikes = resonator.events\n",
    "                start_var = sample * ticks_in_sample\n",
    "                end_var = (sample + 1) * ticks_in_sample\n",
    "                # mask = np.logical_and(ts_spikes >= start_idx, ts_spikes < end_idx)\n",
    "                # ts_spikes = ts_spikes[mask]\n",
    "\n",
    "                start_idx = np.searchsorted(ts_spikes, start_var, side='left')\n",
    "                end_idx = np.searchsorted(ts_spikes, end_var, side='right')\n",
    "                ts_spikes = ts_spikes[start_idx:end_idx]\n",
    "\n",
    "                # make sure all spikes are aligned even though the spikes come from different clocks and different timestamp!\n",
    "                ts_spikes = ts_spikes - (sample * ticks_in_sample)\n",
    "                ts_spikes = ts_spikes * int(self.network_clk // resonator.clk_freq)\n",
    "\n",
    "                neuron_id = ch_id * len(self.map_resonator_to_id) + resonator_id\n",
    "                try:\n",
    "                    result[result_index:result_index + len(ts_spikes), 0] = ts_spikes\n",
    "                    result[result_index:result_index + len(ts_spikes), 1] = neuron_id\n",
    "                except ValueError as e:\n",
    "                    print(f' {ch} - {f} for id = {id} t {trial_id} m {minute_id} s {sample}')\n",
    "                    raise e\n",
    "                result_index += len(ts_spikes)\n",
    "\n",
    "        return result, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "trial = Trial(f'../datasets/EEG_data_for_Mental_Attention_State_Detection/EEG_spikes_clk/', 3)\n",
    "\n",
    "labels_mapper = {\n",
    "    'drowesed': 0,\n",
    "    'focus': 1,\n",
    "    'unfocus': 2,\n",
    "}\n",
    "train_dataset = EEGMentalSpikesDataset(trials=[trial], minutes=[5, 15, 25], time_sample_s=.05, labels_mapper=labels_mapper)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "val_dataset = EEGMentalSpikesDataset(trials=[trial], minutes=[6, 16, 26], time_sample_s=.05, labels_mapper=labels_mapper)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [
    {
     "data": {
      "text/plain": "0.28299784660339355"
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "x, targets = next(iter(train_dataloader))\n",
    "# x, targets = next(iter(train_dataloader))\n",
    "time.time() - st\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 100,  540,  960, 1400, 1820, 2240, 2680, 3100, 3540, 3960],\n        [  40,  460,  880, 1300, 1740, 2160, 2580, 3020, 3440, 3860],\n        [  92,  520,  960, 1380, 1800, 2240, 2660, 3080, 3500, 3940],\n        [  84,  512,  932, 1372, 1792, 2212, 2652, 3072, 3492, 3912],\n        [ 124,  564,  992, 1432, 1852, 2272, 2712, 3140, 3580, 4000],\n        [ 284,  792, 1232, 1652, 2072, 2512, 2932, 3352, 3772, 4212],\n        [ 364,  784, 1292, 1888, 2676, 3096, 3780, 4220, 4988, 5428],\n        [ 196,  636, 1056, 1740, 2180, 2600, 3040, 3460, 3900, 4320],\n        [ 168,  588, 1028, 1448, 1956, 2640, 3428, 3868, 4304, 4744],\n        [ 344,  764, 1184, 1624, 2044, 2464, 2904, 3324, 3764, 4184],\n        [ 340,  760, 1200, 1620, 2040, 2480, 2900, 3340, 3760, 4196],\n        [ 224,  644, 1084, 1504, 1924, 2364, 2784, 3204, 3644, 4064],\n        [ 116,  536,  976, 1396, 1836, 2256, 2676, 3116, 3536, 3956],\n        [  24,  444,  952, 1548, 2232, 2828, 3268, 3688, 4108, 4548],\n        [ 328,  748, 1168, 1608, 2028, 2468, 2888, 3308, 3748, 4176],\n        [ 120,  804, 1488, 1928, 2348, 2788, 3208, 3628, 4068, 4488],\n        [ 340,  780, 1200, 1620, 2060, 2480, 2900, 3340, 3776, 4196],\n        [ 144,  564, 1004, 1440, 1876, 2316, 2736, 3176, 3596, 4016],\n        [ 224,  644, 1084, 1512, 1932, 2372, 2792, 3212, 3652, 4072],\n        [ 404,  824, 1244, 1684, 2104, 2524, 2964, 3384, 3820, 4240],\n        [ 328, 1096, 1780, 2376, 3060, 3656, 4340, 5128, 5568, 5988],\n        [ 184,  624, 1044, 1552, 2236, 2920, 3604, 4288, 4972, 5584],\n        [ 268,  688, 1128, 1548, 1968, 2388, 2828, 3248, 3668, 4088],\n        [ 200,  640, 1060, 1480, 1916, 2356, 2784, 3204, 3644, 4064],\n        [ 184,  868, 1308, 1728, 2168, 2588, 3008, 3448, 3868, 4288],\n        [ 204,  800, 1396, 1992, 2780, 3200, 3884, 4480, 4920, 5340],\n        [ 112,  552,  972, 1392, 1832, 2252, 2512, 3196, 3984, 4404],\n        [ 116,  536,  976, 1396, 1816, 2256, 2676, 3116, 3544, 3984],\n        [ 156,  576, 1004, 1444, 1864, 2284, 2724, 3144, 3388, 3828],\n        [  72,  492,  912, 1352, 1772, 2192, 2632, 3052, 3472, 3912],\n        [ 244,  684, 1112, 1532, 1960, 2400, 2820, 3248, 3668, 4108],\n        [ 592, 1012, 1452, 1608, 2048, 2468, 2908, 3328, 3748, 4188],\n        [ 204,  624, 1044, 1484, 1904, 2324, 2752, 3192, 3612, 4032],\n        [   8,  436,  876, 1304, 1744, 2164, 2600, 3040, 3468, 3908],\n        [ 136,  576,  996, 1416, 1856, 2276, 2712, 3152, 3572, 4012],\n        [  44,  484,  904, 1344, 1764, 2184, 2624, 3044, 3480, 3920],\n        [ 344,  764, 1192, 1628, 2068, 2488, 2908, 3348, 3768, 4188],\n        [ 120,  540,  960, 1400, 1820, 2240, 2680, 3116, 3624, 4308],\n        [ 348,  592, 1276, 2048, 2820, 3260, 3504, 4188, 4888, 5660],\n        [ 156,  596, 1032, 1452, 1892, 2312, 2752, 3172, 3592, 4032],\n        [  20,  460,  880, 1308, 1728, 2168, 2588, 3016, 3444, 3884],\n        [ 412,  832, 1268, 1708, 2136, 2556, 2976, 3416, 3836, 4168],\n        [ 376,  816, 1236, 1676, 2104, 2544, 2972, 3408, 3848, 4268],\n        [ 388,  808, 1248, 1404, 1912, 2352, 3120, 3804, 4400, 4840],\n        [ 344,  588, 1360, 2132, 2572, 2992, 3432, 3852, 4292, 4712],\n        [ 176,  596, 1016, 1456, 1876, 2296, 2736, 3156, 3576, 4016],\n        [ 180,  608, 1028, 1468, 1888, 2308, 2748, 3168, 3596, 4016],\n        [ 276,  696, 1136, 1556, 1984, 2424, 2844, 3264, 3704, 4124],\n        [  60,  480,  916, 1356, 1776, 2196, 2624, 3064, 3484, 3904],\n        [ 104,  540,  980, 1400, 1820, 2260, 2688, 3116, 3556, 3976],\n        [ 248,  668, 1108, 1528, 1948, 2388, 2808, 3248, 3668, 4096],\n        [ 408,  836, 1276, 1696, 2116, 2536, 2976, 3396, 3816, 4236],\n        [ 212,  632, 1060, 1500, 1936, 2376, 2796, 3216, 3656, 4076],\n        [ 396,  816, 1412, 2192, 2632, 3052, 3488, 3928, 4364, 4784],\n        [ 192,  632, 1052, 1472, 1912, 2332, 2752, 3192, 3612, 4048],\n        [ 376,  816, 1236, 1656, 2096, 2516, 2936, 3356, 3796, 4216],\n        [ 144,  564, 1000, 1428, 1868, 2288, 2708, 3128, 3548, 3968],\n        [ 324,  744, 1164, 1604, 2024, 2452, 2892, 3312, 3752, 4172],\n        [ 316,  736, 1176, 1596, 2016, 2452, 2892, 3312, 3732, 4168],\n        [ 128,  724, 1408, 2004, 2444, 2864, 3652, 4072, 4512, 4668],\n        [ 384,  824,  980, 1576, 2172, 2612, 3032, 3820, 4240, 4680],\n        [ 240,  660, 1100, 1520, 1852, 2448, 3044, 3728, 4324, 4764],\n        [  28,  464,  884, 1324, 1752, 2172, 2612, 3040, 3460, 4056],\n        [ 264,  684, 1104, 1544, 1964, 2384, 2824, 3244, 3664, 4104],\n        [ 352,  772, 1192, 1612, 2052, 2472, 2900, 3340, 3760, 4196],\n        [ 360,  780, 1220, 1640, 2060, 2500, 2920, 3340, 3780, 4200],\n        [ 428, 1216, 1372, 1968, 2748, 3188, 3608, 4028, 4468, 4624],\n        [  28,  448,  888, 1308, 1728, 2168, 2588, 3028, 3184, 3624],\n        [ 412,  852, 1280, 1700, 2136, 2576, 2996, 3424, 3860, 4280],\n        [  96,  516,  956, 1376, 1796, 2480, 3164, 3848, 4532, 5216],\n        [ 264,  684, 1104, 1524, 1964, 2384, 2804, 3224, 3664, 4084],\n        [ 192,  612, 1032, 1472, 1900, 2320, 2760, 3180, 3608, 4048],\n        [ 164,  584, 1024, 1444, 1864, 2304, 2732, 3172, 3592, 4012],\n        [ 324,  764, 1184, 1604, 2044, 2480, 2900, 3340, 3760, 4180],\n        [   8,  428,  848, 1288, 1708, 2128, 2568, 2988, 3408, 3848],\n        [ 220,  660, 1080, 1508, 1928, 2368, 2796, 3216, 3656, 4076],\n        [  96,  536,  964, 1404, 1840, 2348, 3120, 3912, 4068, 4508],\n        [ 136,  572, 1012, 1432, 1852, 2292, 2712, 3132, 3568, 4008],\n        [ 424,  844, 1280, 1720, 2140, 2560, 3000, 3428, 3856, 4296],\n        [ 536,  976, 1412, 1832, 2272, 2692, 3128, 3636, 4232, 4828],\n        [ 616, 1036, 1476, 1896, 2316, 2756, 3176, 3596, 4036, 4464],\n        [ 328,  748, 1188, 1608, 2028, 2448, 2888, 3316, 3736, 4176],\n        [ 416,  836, 1276, 1696, 2136, 2556, 2996, 3424, 3864, 4284],\n        [ 148,  588, 1008, 1428, 1868, 2288, 2708, 3148, 3568, 4008],\n        [   8,  448,  868, 1376, 1972, 2656, 3260, 3700, 4120, 4628],\n        [ 424,  844, 1264, 1704, 2124, 2544, 2984, 3412, 3832, 4272],\n        [  92,  532,  952, 1372, 1812, 2240, 2660, 3100, 3520, 3940],\n        [ 400,  840, 1260, 1680, 2120, 2540, 2960, 3400, 3820, 4240],\n        [ 240,  660, 1100, 1520, 1940, 2380, 2800, 3240, 3676, 4096],\n        [  84,  504,  924, 1364, 1784, 2204, 2624, 3044, 3480, 3920],\n        [ 412,  832, 1252, 2040, 2460, 2900, 3320, 3564, 4160, 4844],\n        [ 324,  744, 1164, 1604, 2024, 2444, 2880, 3320, 3756, 4176],\n        [ 228,  912, 1596, 2192, 2876, 3656, 4096, 4516, 4936, 5376],\n        [ 344,  784, 1204, 1624, 2064, 2492, 2912, 3352, 3772, 4192],\n        [ 384,  804, 1400, 2084, 2768, 3364, 3804, 4224, 4660, 5432],\n        [ 248,  684, 1104, 1524, 1964, 2384, 2804, 3224, 3664, 4084],\n        [ 148,  568, 1008, 1428, 1848, 2288, 2708, 3128, 3568, 3988],\n        [ 348,  768, 1188, 1628, 2056, 2476, 2916, 3336, 3756, 4196],\n        [ 512,  756, 1548, 1968, 2408, 2740, 3532, 3952, 4392, 4812],\n        [ 232,  652, 1248, 1852, 2544, 3228, 3912, 4700, 5136, 5576]])"
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, :10, 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [],
   "source": [
    "y = np.zeros((x.shape[0], 25*14))\n",
    "indices = (x[:, :, 0] == 100).nonzero()\n",
    "features = x[indices[:, 0], indices[:, 1], 1]\n",
    "y[indices[:, 0], features] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([  0, 260, 272])"
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, x[0, :, 0] == 100, 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5efD9OHkvVU"
   },
   "source": [
    "## Define The Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "id": "I8WkfQIneZat",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6eb4efeb-bc6c-4dc7-d9e4-66ec56617f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.0 GB\n",
      "Max memory allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Define Network\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, network_clk, sample_time_s, num_inputs, beta=.95):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network_clk = network_clk\n",
    "        self.sample_time_s = sample_time_s\n",
    "        self.steps = 5000 #int(self.network_clk * self.sample_time_s)\n",
    "\n",
    "        self.fc1 = nn.Linear(num_inputs, num_inputs * 3)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_inputs * 3, len(labels_mapper))\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def event_ts_to_spikes(self, events_ts, t):\n",
    "        x = np.zeros((events_ts.shape[0], 25*14))\n",
    "        indices = (events_ts[:, :, 0] == t).nonzero()\n",
    "        features = events_ts[indices[:, 0], indices[:, 1], 1]\n",
    "        x[indices[:, 0], features] = 1\n",
    "        return torch.tensor(x, requires_grad=True).float()\n",
    "\n",
    "    def forward(self, events_ts):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif1.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "        for i in range(self.steps):\n",
    "            spikes = self.event_ts_to_spikes(events_ts, i)\n",
    "            spikes = spikes.to(device)\n",
    "            cur1 = self.fc1(spikes)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk_rec.append(spk2)\n",
    "            mem_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)\n",
    "        \n",
    "# Load the network onto CUDA if available\n",
    "net = SNN(train_dataset.network_clk, sample_time_s=.05, num_inputs=14*25).to(device)\n",
    "\n",
    "gpu_mem_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "id": "DH_JOR2qgOvu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "52a2c129-7210-4ecb-9aa3-9a9c6deaa83b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.0 GB\n",
      "Max memory allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "def train_printer(\n",
    "    data, targets, epoch,\n",
    "    counter, iter_counter,\n",
    "        loss_hist, test_loss_hist, test_data, test_targets):\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "gpu_mem_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Km_5-ekz72U"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9ZF1z0XzsZ9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "outputId": "11a82f41-d7c6-4deb-aa6d-b1be76ab977c",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "val_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "\n",
    "    with tqdm(total=len(train_dataloader) + len(val_dataloader)) as pbar:\n",
    "        # Minibatch training loop\n",
    "        for data, targets in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # targets = F.one_hot(targets, num_classes=len(labels_mapper))\n",
    "\n",
    "            # forward pass\n",
    "            net.train()\n",
    "            spk_rec, mem_rec = net(data)\n",
    "\n",
    "            _, idx = spk_rec.sum(dim=0).max(1)\n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(net.steps):\n",
    "                loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Val set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for data, targets in train_dataloader:\n",
    "                val_data = val_data.to(device)\n",
    "                val_targets = val_targets.to(device)\n",
    "\n",
    "                # Val set forward pass\n",
    "                val_spk, val_mem = net(val_data)\n",
    "\n",
    "                _, idx = val_spk.sum(dim=0).max(1)\n",
    "                val_acc = np.mean((val_targets == idx).detach().cpu().numpy())\n",
    "\n",
    "                # Val set loss\n",
    "                val_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "                for step in range(net.steps):\n",
    "                    val_loss += loss(val_mem[step], val_targets)\n",
    "                val_loss_hist.append(val_loss.item())\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Print train/val loss/accuracy\n",
    "            if counter % 1 == 0:\n",
    "              print(f'train acc {train_acc}, val acc {val_acc}')\n",
    "              train_printer(\n",
    "                  data, targets, epoch,\n",
    "                  counter, iter_counter,\n",
    "                  loss_hist, val_loss_hist,\n",
    "                  val_data, val_targets)\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ANN"
   ],
   "metadata": {
    "id": "zSChQREpKCbV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(14, 32, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 3))\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.fc1 = nn.Linear(179456, 1024)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.dtype)\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "cnet = CNN().to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnet.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "gpu_mem_state()"
   ],
   "metadata": {
    "id": "hhtgrWTCopta",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7ed5a1e0-0076-4d84-b38c-493cf0473d7d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory allocated: 0.6859326362609863 GB\n",
      "Max memory allocated: 0.6859326362609863 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# del data\n",
    "# del targets\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gpu_mem_state()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AxsuUleTneA",
    "outputId": "8688ecc6-d773-4231-ccdb-0aec30b913b3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory allocated: 0.6859326362609863 GB\n",
      "Max memory allocated: 0.6859326362609863 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "# Define the train and validation loops\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    for data, target in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = 100. * train_correct / len(train_loader.dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in  tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            val_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = 100. * val_correct / len(val_loader.dataset)\n",
    "    return val_loss, val_acc\n",
    "\n",
    "# Train and validate the CNN\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss, train_acc = train(cnet, train_loader, optimizer, loss, device)\n",
    "    val_loss, val_acc = validate(cnet, val_loader, loss, device)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.6f}, Val Acc: {val_acc:.2f}%')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "An4-iHRsQSuE",
    "outputId": "c6853dd9-d92f-40f3-f0c8-aa45de8d4f96"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train Loss: 0.047621, Train Acc: 32.21%, Val Loss: 0.046404, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2: Train Loss: 0.045774, Train Acc: 34.48%, Val Loss: 0.046393, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3: Train Loss: 0.045769, Train Acc: 34.48%, Val Loss: 0.046397, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4: Train Loss: 0.045768, Train Acc: 34.48%, Val Loss: 0.046395, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train Loss: 0.045768, Train Acc: 34.48%, Val Loss: 0.046392, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6: Train Loss: 0.045768, Train Acc: 34.48%, Val Loss: 0.046390, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-c08d150d81f3>\u001B[0m in \u001B[0;36m<cell line: 38>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0mn_epochs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_epochs\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m     \u001B[0mtrain_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_acc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m     \u001B[0mval_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_acc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalidate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Epoch {epoch}: Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.6f}, Val Acc: {val_acc:.2f}%'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-7-c08d150d81f3>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, optimizer, criterion, device)\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mtrain_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mtrain_correct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdesc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Training\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mleave\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1176\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1177\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1178\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0miterable\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1179\u001B[0m                 \u001B[0;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1180\u001B[0m                 \u001B[0;31m# Update and possibly print the progressbar.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    632\u001B[0m                 \u001B[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    633\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 634\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    635\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    676\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    677\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 678\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    679\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    680\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory_device\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitems__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitems__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-884cfb72a501>\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, id)\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[0mdata1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{self.samples_folder}/{fname}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'spikes'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m       \u001B[0mdata2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{self.samples_folder}/{trial}_{start_time_ms+3000}.npz'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'spikes'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mFileNotFoundError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m       \u001B[0mdata2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{self.samples_folder}/{trial}_{start_time_ms+2999}.npz'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'spikes'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    241\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mmagic\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMAGIC_PREFIX\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    242\u001B[0m                 \u001B[0mbytes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 243\u001B[0;31m                 return format.read_array(bytes,\n\u001B[0m\u001B[1;32m    244\u001B[0m                                          \u001B[0mallow_pickle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mallow_pickle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    245\u001B[0m                                          pickle_kwargs=self.pickle_kwargs)\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001B[0m in \u001B[0;36mread_array\u001B[0;34m(fp, allow_pickle, pickle_kwargs)\u001B[0m\n\u001B[1;32m    776\u001B[0m                     \u001B[0mread_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_read_count\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcount\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    777\u001B[0m                     \u001B[0mread_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mread_count\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitemsize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 778\u001B[0;31m                     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_read_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mread_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"array data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    779\u001B[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001B[1;32m    780\u001B[0m                                                              count=read_count)\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001B[0m in \u001B[0;36m_read_bytes\u001B[0;34m(fp, size, error_template)\u001B[0m\n\u001B[1;32m    905\u001B[0m         \u001B[0;31m# done about that.  note that regular files can't be non-blocking\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    906\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 907\u001B[0;31m             \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    908\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    909\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/zipfile.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, n)\u001B[0m\n\u001B[1;32m    925\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_offset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    926\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_eof\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 927\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_read1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    928\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    929\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_readbuffer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/zipfile.py\u001B[0m in \u001B[0;36m_read1\u001B[0;34m(self, n)\u001B[0m\n\u001B[1;32m   1001\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compress_type\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mZIP_DEFLATED\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1002\u001B[0m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMIN_READ_SIZE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1003\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_decompressor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecompress\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1004\u001B[0m             self._eof = (self._decompressor.eof or\n\u001B[1;32m   1005\u001B[0m                          \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compress_left\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(cnet.state_dict(), '\"/content/drive/MyDrive/SNN-Thesis/cnn_trained_model.pth')"
   ],
   "metadata": {
    "id": "8yIeqKAtT7-S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "api_key = '8236F572-BB36-431D-A64C-3A21B2751024'\n",
    "\n",
    "symbol = 'BTC_USDT'.upper().replace('-', '_')\n",
    "trades = []\n",
    "endpoint = f'https://rest.coinapi.io/v1/trades/BINANCEFTS_PERP_{symbol}/history'\n",
    "params = {\n",
    "    'apikey': api_key,\n",
    "    'time_start': '2023-03-21T08:59:54+00:00',\n",
    "    'limit': 100,\n",
    "}\n",
    "response = requests.get(endpoint, params=params, headers={'Accept': 'application/json'})\n"
   ],
   "metadata": {
    "id": "aLxavaeFWIKf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYbg_gWoCuVj",
    "outputId": "4b31814f-64bf-42e1-9379-08b73bb262f3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Response [500]>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Dcfgz2n9CwBL"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "machine_shape": "hm"
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
